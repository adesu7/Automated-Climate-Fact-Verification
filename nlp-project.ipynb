{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.parsing import preprocess_string\n",
    "# preprocess the data\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "claim = 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('data/train-claims.json') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open('data/dev-claims.json') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "with open('data/test-claims-unlabelled.json') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "with open('data/evidence.json') as f:\n",
    "    evidence_data = json.load(f)\n",
    "\n",
    "evidences = list(evidence_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_text</th>\n",
       "      <th>evidence_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>At very high concentrations (100 times atmosph...</td>\n",
       "      <td>DISPUTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>Plants can grow as much as 50 percent faster i...</td>\n",
       "      <td>DISPUTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>Higher carbon dioxide concentrations will favo...</td>\n",
       "      <td>DISPUTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El Niño drove record highs in global temperatu...</td>\n",
       "      <td>While ‘climate change’ can be due to natural f...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>El Niño drove record highs in global temperatu...</td>\n",
       "      <td>This acceleration is due mostly to human-cause...</td>\n",
       "      <td>REFUTES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4117</th>\n",
       "      <td>But abnormal temperature spikes in February an...</td>\n",
       "      <td>The coastline sees significantly mild temperat...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4118</th>\n",
       "      <td>Sending oscillating microwaves from an antenna...</td>\n",
       "      <td>Dielectric heating, also known as electronic h...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4119</th>\n",
       "      <td>Sending oscillating microwaves from an antenna...</td>\n",
       "      <td>An example is absorption or emission of radio ...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>Sending oscillating microwaves from an antenna...</td>\n",
       "      <td>Water, fat, and other substances in the food a...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4121</th>\n",
       "      <td>Sending oscillating microwaves from an antenna...</td>\n",
       "      <td>A microwave oven passes microwave radiation at...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4122 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             claim_text  \\\n",
       "0     Not only is there no scientific evidence that ...   \n",
       "1     Not only is there no scientific evidence that ...   \n",
       "2     Not only is there no scientific evidence that ...   \n",
       "3     El Niño drove record highs in global temperatu...   \n",
       "4     El Niño drove record highs in global temperatu...   \n",
       "...                                                 ...   \n",
       "4117  But abnormal temperature spikes in February an...   \n",
       "4118  Sending oscillating microwaves from an antenna...   \n",
       "4119  Sending oscillating microwaves from an antenna...   \n",
       "4120  Sending oscillating microwaves from an antenna...   \n",
       "4121  Sending oscillating microwaves from an antenna...   \n",
       "\n",
       "                                          evidence_text            label  \n",
       "0     At very high concentrations (100 times atmosph...         DISPUTED  \n",
       "1     Plants can grow as much as 50 percent faster i...         DISPUTED  \n",
       "2     Higher carbon dioxide concentrations will favo...         DISPUTED  \n",
       "3     While ‘climate change’ can be due to natural f...          REFUTES  \n",
       "4     This acceleration is due mostly to human-cause...          REFUTES  \n",
       "...                                                 ...              ...  \n",
       "4117  The coastline sees significantly mild temperat...  NOT_ENOUGH_INFO  \n",
       "4118  Dielectric heating, also known as electronic h...         SUPPORTS  \n",
       "4119  An example is absorption or emission of radio ...         SUPPORTS  \n",
       "4120  Water, fat, and other substances in the food a...         SUPPORTS  \n",
       "4121  A microwave oven passes microwave radiation at...         SUPPORTS  \n",
       "\n",
       "[4122 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to get the evidence for a claim\n",
    "def get_evidence(evidence_id):\n",
    "    return evidence_data[evidence_id]\n",
    "\n",
    "# function to get the claim text\n",
    "def get_claim_text(claim_id):\n",
    "    return train_data[claim_id]['claim_text']\n",
    "\n",
    "# function to get the label for a claim\n",
    "def get_label(claim_id):\n",
    "    return train_data[claim_id]['claim_label']\n",
    "\n",
    "# function to create a dataframe with the claim, evidence and label\n",
    "def create_dataframe(data):\n",
    "    rows = []\n",
    "    for claim_id in data:\n",
    "        for evidence_id in train_data[claim_id]['evidences']:\n",
    "            row = {\n",
    "                'claim_text': get_claim_text(claim_id),\n",
    "                'evidence_text': get_evidence(evidence_id=evidence_id),\n",
    "                'label': get_label(claim_id)\n",
    "            }\n",
    "            rows.append(row)\n",
    "    df = pd.concat([pd.DataFrame([row]) for row in rows], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# create the dataframes\n",
    "train_df = create_dataframe(train_data)\n",
    "#dev_df = create_dataframe(dev_data)\n",
    "\n",
    "#print entire evidence for a claim of the train_df\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # remove stopwords\n",
    "    text = ' '.join([lemma.lemmatize(word) for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208827\n"
     ]
    }
   ],
   "source": [
    "#use tfidf to find the most important evidence for a claim\n",
    "\n",
    "claim = 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "preprocessed_corpus = [' '.join([lemma.lemmatize(word.lower()) for word in sentence.split() if word not in stop_words]) for key, sentence in evidence_data.items()]\n",
    "#preprocessed_corpus = preprocess_documents(evidences)\n",
    "\n",
    "# get the most important evidence for a claim\n",
    "#get_most_important_evidence(claim, evidence_data)\n",
    "\n",
    "print(len(preprocessed_corpus))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Higher atmospheric CO2 concentrations have led to an increase in dissolved CO2, which causes ocean acidification.',\n",
       "  'There is also a close correlation between CO2 and temperature, where CO2 has a strong control over global temperatures in Earth history.',\n",
       "  'The concentration of CO2 in the flue gas is an important key to determine the efficiency of CO2 capture technology.',\n",
       "  'It is expected that most ecosystems will be affected by higher atmospheric CO2 levels and higher global temperatures.',\n",
       "  'CO2).',\n",
       "  \"The increased radiative forcing due to increased CO2 in the Earth's atmosphere is based on the physical properties of CO2 and the non-saturated absorption windows where CO2 absorbs outgoing long-wave energy.\",\n",
       "  'Coal, being mostly carbon, emits a lot of CO2 when burnt: it has a high CO2 emission intensity.'],\n",
       " ['evidence-66273',\n",
       "  'evidence-98914',\n",
       "  'evidence-256886',\n",
       "  'evidence-364767',\n",
       "  'evidence-668884',\n",
       "  'evidence-985452',\n",
       "  'evidence-1162945'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_relevant_evidence_tfidf(claim, vectors, evidences, top_n=7):\n",
    "    # Process the claim\n",
    "    processed_claim = preprocess_text(claim)\n",
    "\n",
    "    # Get the tfidf vector for the claim\n",
    "    claim_vector = vectorizer.transform([processed_claim])\n",
    "\n",
    "    # Get the cosine similarity between the claim and the evidence\n",
    "\n",
    "    cosine_similarity(claim_vector, vectors)\n",
    "\n",
    "    # Get the top 5 most similar evidence\n",
    "    top_n_evidences = cosine_similarity(claim_vector, vectors).argsort()[0][-top_n:]\n",
    "    # Get the top 5 most similar evidence unprocessed\n",
    " \n",
    "    filtered_items = [item for i, item in enumerate(evidence_data.items()) if i in top_n_evidences]\n",
    "    most_similar_evidence_keys, most_similar_evidence = zip(*filtered_items)\n",
    "\n",
    "    most_similar_evidence_keys = list(most_similar_evidence_keys)\n",
    "    most_similar_evidence = list(most_similar_evidence)\n",
    "    return most_similar_evidence, most_similar_evidence_keys\n",
    "\n",
    "get_relevant_evidence_tfidf(claim, vectors, evidence_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DOC2VEC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#processed_corpus = [doc.split(\" \") for doc in preprocessed_corpus]\n",
    "#tokenised_corpus = preprocess_documents(evidences)\n",
    "# Train Doc2Vec model on preprocessed corpus\n",
    "#documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenised_corpus)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=45, window=2, min_count=1, workers=8)\n",
    "model.save(\"models/doc2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Get the most similar evidence document to a claim\n",
    "def get_most_similar_evidence(claim, evidence_data, model, top_n=10):\n",
    "    # Preprocess claim\n",
    "    processed_claim = preprocess_string(claim)\n",
    "\n",
    "    # Get claim vector\n",
    "    claim_vector = model.infer_vector(processed_claim)\n",
    "    \n",
    "    # similar docs\n",
    "    similarity = model.dv.most_similar(claim_vector, topn=top_n)\n",
    "    print(similarity)\n",
    "    #top 5 indices\n",
    "    top_n_indices = [index for index, score in similarity]\n",
    "\n",
    "    # top 5 evidences\n",
    "    top_n_evidences = [evidence_data[index] for index in top_n_indices]\n",
    "    return  top_n_indices, top_n_evidences\n",
    "\n",
    "model = Doc2Vec.load(\"models/doc2vec.model\")\n",
    "claim = 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'\n",
    "top_n_indices, top_n_evidences = get_most_similar_evidence(claim, evidences, model, top_n=10)\n",
    "\n",
    "print(\"Claim: \", claim)\n",
    "print(\"Top 10 results:\")\n",
    "for i, evidence in enumerate(top_n_evidences):\n",
    "    print(f'{i+1}. {evidence}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BM25**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess data and get BM25 vectors for evidence corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenised_corpus = preprocess_documents(evidences)\n",
    "bm25 = BM25Okapi(tokenised_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.',\n",
       "  'This scientific evidence comes from many sources but is presented in detail in the Millennium Ecosystem Assessment and the planetary boundaries framework.',\n",
       "  'The concentration of pollutants in the flue gas is higher, making separation easier.',\n",
       "  'The whitebark pine ecosystem in these high elevations plays many essential roles, providing support to plant and animal life.',\n",
       "  'Diverse forest plant life exists in the park and the area supports several wild animal species.',\n",
       "  'They study the various animals and plants that live within an ecosystem and the relationship between the two.',\n",
       "  'He helped to organize cultural life in Theresienstadt concentration camp.'],\n",
       " ['evidence-442946',\n",
       "  'evidence-528208',\n",
       "  'evidence-585692',\n",
       "  'evidence-588973',\n",
       "  'evidence-791682',\n",
       "  'evidence-909029',\n",
       "  'evidence-1183281'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_relevant_evidence_bm25(claim, evidence_data, bm25, top_n=7):\n",
    "    \"\"\" \"\"\"\n",
    "    processed_claim = preprocess_string(claim)\n",
    "  \n",
    "    # Get BM25 scores for each evidence sentence\n",
    "    bm25_scores = bm25.get_scores(processed_claim)\n",
    "    # Get top 5 most similar evidence sentences\n",
    "    top_n_indices = bm25_scores.argsort()[::-1][:top_n]\n",
    "    filtered_items = [item for i, item in enumerate(evidence_data.items()) if i in top_n_indices]\n",
    "    most_similar_evidence_keys, most_similar_evidence = zip(*filtered_items)\n",
    "\n",
    "    most_similar_evidence_keys = list(most_similar_evidence_keys)\n",
    "    most_similar_evidence = list(most_similar_evidence)\n",
    "    #best_evidence = bm25.get_top_n(processed_claim, evidences, n=top_n)\n",
    "    # Print results\n",
    "    #print(best_evidence)\n",
    "    # print(\"------BM25 Evidence-----\")\n",
    "    # for i, evidence in enumerate(most_similar_evidence):\n",
    "    #     print(f'{i+1}. {evidence}')\n",
    "    return most_similar_evidence, most_similar_evidence_keys\n",
    "\n",
    "get_relevant_evidence_bm25(claim=claim, evidence_data=evidence_data, bm25=bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT ATTEMPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the capital of France?\"\n",
    "passages = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"France is famous for its wine and cheese.\",\n",
    "    \"The Louvre is a famous museum in Paris.\",\n",
    "    \"The French Revolution started in 1789.\",\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.55245208740234 CO2).\n",
      "95.90694427490234 It is expected that most ecosystems will be affected by higher atmospheric CO2 levels and higher global temperatures.\n",
      "93.92660522460938 Higher atmospheric CO2 concentrations have led to an increase in dissolved CO2, which causes ocean acidification.\n",
      "93.88444519042969 There is also a close correlation between CO2 and temperature, where CO2 has a strong control over global temperatures in Earth history.\n",
      "92.76532745361328 Coal, being mostly carbon, emits a lot of CO2 when burnt: it has a high CO2 emission intensity.\n",
      "91.64056396484375 The concentration of CO2 in the flue gas is an important key to determine the efficiency of CO2 capture technology.\n",
      "91.06651306152344 The increased radiative forcing due to increased CO2 in the Earth's atmosphere is based on the physical properties of CO2 and the non-saturated absorption windows where CO2 absorbs outgoing long-wave energy.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from sentence_transformers.util import semantic_search\n",
    "#query = \"How many people live in London?\"\n",
    "#docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "mps_device = torch.device(\"mps\")\n",
    "#Load the model\n",
    "model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-tas-b',device=mps_device)\n",
    "#model = SentenceTransformer('bounedjarr/sgpt-finetuned-natcat',device=mps_device)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"climatebert/distilroberta-base-climate-f\")\n",
    "\n",
    "#Encode query and documents\n",
    "query_emb = model.encode(claim)\n",
    "doc_emb = model.encode(most_similar_evidence)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(most_similar_evidence, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n",
    "\n",
    "# results = semantic_search(query_emb, doc_emb, top_k=5) \n",
    "# for result in results[0]:\n",
    "#     print(result)\n",
    "#     evidence_id = result['corpus_id']\n",
    "#     sim_score = result['score']\n",
    "#     print(most_similar_evidence[evidence_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'corpus_id': 12, 'score': 0.9166639447212219}\n",
    "Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.\n",
    "{'corpus_id': 5, 'score': 0.9083583354949951}\n",
    "At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.\n",
    "{'corpus_id': 1, 'score': 0.8610204458236694}\n",
    "The concentration of pollutants in the flue gas is higher, making separation easier.\n",
    "{'corpus_id': 16, 'score': 0.7930693030357361}\n",
    "This, along with higher temperatures, would mean a higher equilibrium concentration of in the air.\n",
    "{'corpus_id': 15, 'score': 0.7820011973381042}\n",
    "Sustainable agriculture is the cultivation of plant and animal materials in a manner that preserves plant and animal ecosystems and that can improve soil health and soil fertility over the long term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saved embedding of evidence document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define batch size\n",
    "# batch_size = 1000\n",
    "\n",
    "# # Encode documents in batches\n",
    "# doc_emb = []\n",
    "# for i in range(0, len(evidences), batch_size):\n",
    "#   start_time = time.time()\n",
    "#   batch = evidences[i:i+batch_size]\n",
    "#   batch_emb = model.encode(batch)\n",
    "#   doc_emb.append(batch_emb)\n",
    "#   print(f\"Batch {i} processed time taken {time.time()- start_time}\")\n",
    "# doc_emb = np.concatenate(doc_emb, axis=0)\n",
    "# # Save document embeddings as a NumPy array\n",
    "# np.save('doc_emb.npy', doc_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb = model.encode(claim)\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(evidences, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "threshold = 9\n",
    "#Output passages & scores\n",
    "for i, (doc, score) in enumerate(doc_score_pairs):\n",
    "  if i > 10:\n",
    "    break\n",
    "  print(score, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.', 'Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.', 'Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.']\n"
     ]
    }
   ],
   "source": [
    "train_df.iloc[0]['evidence_text']\n",
    "temp_evidences= ['At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.', \n",
    "'Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.', \n",
    "'Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.']\n",
    "print(temp_evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at climatebert/distilroberta-base-climate-f were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at climatebert/distilroberta-base-climate-f and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1947, -0.2067]])\n",
      "['supports']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "mps_device = torch.device(\"mps\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"climatebert/distilroberta-base-climate-f\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-tas-b\")\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(\"bounedjarr/sgpt-finetuned-natcat\")\n",
    "\n",
    "#model.to(mps_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-f\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/msmarco-distilbert-base-tas-b\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"bounedjarr/sgpt-finetuned-natcat\")\n",
    "\n",
    "claim = 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'\n",
    "evidence = \"Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.\"\n",
    "\n",
    "inputs = tokenizer(claim, evidence, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "#inputs.to(mps_device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model(**inputs).logits\n",
    "    print(scores)\n",
    "    label_mapping = ['supports','refutes','not enough info','disputed']\n",
    "    labels = [label_mapping[score_max] for score_max in scores.argmax(dim=1)]\n",
    "    print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REFUTES\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def majority_vote_prediction(claim, relevant_evidences,model_name=\"amandakonet/climatebert-fact-checking\"):\n",
    "    #mps_device = torch.device(\"mps\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    #model.to(mps_device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    label_mapping = ['supports','refutes','not_enough_info','disputed']\n",
    "    predictions = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for evidence in relevant_evidences:\n",
    "            inputs = tokenizer(claim, evidence, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            #inputs.to(mps_device)\n",
    "            scores = model(**inputs).logits\n",
    "            label = label_mapping[scores.argmax(dim=1).item()]\n",
    "            predictions.append(label.upper())\n",
    "\n",
    "    majority_vote = max(set(predictions), key=predictions.count)\n",
    "    #print(claim, majority_vote)\n",
    "    return majority_vote\n",
    "\n",
    "# Example usage:\n",
    "claim = 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.'\n",
    "evidences = ['At very high concentrations (100 times atmospheric concentration, or greater), carbon dioxide can be toxic to animal life, so raising the concentration to 10,000 ppm (1%) or higher for several hours will eliminate pests such as whiteflies and spider mites in a greenhouse.', \n",
    "'Plants can grow as much as 50 percent faster in concentrations of 1,000 ppm CO 2 when compared with ambient conditions, though this assumes no change in climate and no limitation on other nutrients.', \n",
    "'Higher carbon dioxide concentrations will favourably affect plant growth and demand for water.']\n",
    "result = majority_vote_prediction(claim, evidences)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evidence_make_prediction(claim_data, evidence_data, vectors, bm25, top_n, mode):\n",
    "    results = {}\n",
    "\n",
    "    for claim_id in claim_data:\n",
    "        claim = claim_data[claim_id]['claim_text']\n",
    "        if mode == \"tfidf\":\n",
    "            relevant_evidences, relevant_evidence_keys = get_relevant_evidence_tfidf(claim, vectors, evidence_data, top_n)\n",
    "        elif mode == \"bm25\":\n",
    "            relevant_evidences, relevant_evidence_keys = get_relevant_evidence_bm25(claim, evidence_data, bm25, top_n)\n",
    "\n",
    "        predicted_label = majority_vote_prediction(claim, relevant_evidences)\n",
    "        \n",
    "        results[claim_id] = {\n",
    "            \"claim_text\": claim,\n",
    "            \"claim_label\": predicted_label,\n",
    "            \"evidences\": relevant_evidence_keys\n",
    "        }\n",
    "        \n",
    "    return results\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_results_bm25 = get_evidence_make_prediction(dev_data, evidence_data, vectors, bm25, top_n=7, mode=\"bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_results_tfidf = get_evidence_make_prediction(dev_data, evidence_data, vectors, bm25, top_n=7, mode=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predicted_results_bm25.json', 'w') as fp:\n",
    "    json.dump(predicted_results_bm25, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predicted_results_tfidf.json', 'w') as fp:\n",
    "    json.dump(predicted_results_tfidf, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evidence Retrieval F-score (F)    = 0.07668240850059031\n",
      "Claim Classification Accuracy (A) = 0.4935064935064935\n",
      "Harmonic Mean of F and A          = 0.13273940057250957\n"
     ]
    }
   ],
   "source": [
    "!python data/eval.py --predictions predicted_results_tfidf.json --groundtruth data/dev-claims.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "437e527ee4babf76d44c3f9f00f918fc2f1537b9b44b1e1cf92c55d1d05ae7c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
